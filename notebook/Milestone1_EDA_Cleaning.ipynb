{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697268c7",
   "metadata": {},
   "source": [
    "# NYC Traffic Crashes Analysis and Data Preparation\n",
    "\n",
    "This notebook performs comprehensive analysis of NYC traffic crashes data, preparing it for visualization in a web application. The analysis includes data cleaning, integration, and exploratory analysis of crash patterns across New York City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ed1b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Crashes file 'Motor_Vehicle_Collisions_-_Crashes_20251111.csv' not found.\n",
      "Initial Crashes Data Shape: (0, 0)\n",
      "\n",
      "--- Initial Data Info (Sample) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n",
      "\n",
      "--- Top Missing Values (Crashes Dataset) ---\n",
      "Series([], dtype: float64)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'BOROUGH'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# --- Initial Visualization (Crashes by Borough) ---\u001b[39;00m\n\u001b[32m     31\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m borough_counts = \u001b[43mdf_crashes\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBOROUGH\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.fillna(\u001b[33m'\u001b[39m\u001b[33mUNKNOWN\u001b[39m\u001b[33m'\u001b[39m).value_counts()\n\u001b[32m     33\u001b[39m sns.barplot(x=borough_counts.index, y=borough_counts.values, palette=\u001b[33m'\u001b[39m\u001b[33mviridis\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mInitial Crash Counts by Borough\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tarek Metwally\\Documents\\GitHub\\Viusals\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tarek Metwally\\Documents\\GitHub\\Viusals\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'BOROUGH'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# --- Setup and File Paths ---\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "crashes_file = 'Motor_Vehicle_Collisions_-_Crashes_20251111.csv'\n",
    "persons_file = 'Motor_Vehicle_Collisions_-_Person_20251111.csv' # Kept for loading in Cell 3\n",
    "\n",
    "# --- Load raw data ---\n",
    "try:\n",
    "    df_crashes_raw = pd.read_csv(crashes_file, low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Crashes file '{crashes_file}' not found.\")\n",
    "    df_crashes_raw = pd.DataFrame()\n",
    "\n",
    "print(\"Initial Crashes Data Shape:\", df_crashes_raw.shape)\n",
    "df_crashes = df_crashes_raw.copy()\n",
    "\n",
    "# --- Print Info and Missing Summary ---\n",
    "print(\"\\n--- Initial Data Info (Sample) ---\")\n",
    "df_crashes.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "print(\"\\n--- Top Missing Values (Crashes Dataset) ---\")\n",
    "missing_summary = df_crashes.isnull().sum().sort_values(ascending=False).head(15)\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# --- Initial Visualization (Crashes by Borough) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "borough_counts = df_crashes['BOROUGH'].fillna('UNKNOWN').value_counts()\n",
    "sns.barplot(x=borough_counts.index, y=borough_counts.values, palette='viridis')\n",
    "plt.title('Initial Crash Counts by Borough')\n",
    "plt.ylabel('Number of Crashes')\n",
    "plt.xlabel('Borough')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a510b",
   "metadata": {},
   "source": [
    "# 2. Pre-Integration Cleaning Justification and Execution\n",
    "\n",
    "This cell performs robust cleaning on the **Crashes** dataset before integration, including missing value handling, outlier removal, and data standardization.\n",
    "\n",
    "## Justification for Cleaning Decisions (REQUIRED DOCUMENTATION)\n",
    "\n",
    "### 1. Handling Missing Values\n",
    "\n",
    "* **Column Dropping (High Null Count):** Any column with **over 50%** of its values missing is dropped (e.g., secondary vehicle factors). These columns offer insufficient data integrity for reliable analysis.\n",
    "* **Imputation Strategy:**\n",
    "    * **Numeric Data (Injuries/Fatalities):** Missing numeric values are imputed using the **Median**. The median is preferred over the mean because collision counts are highly skewed (many zeros, few high values), making the median a more robust and representative measure.\n",
    "    * **Categorical Data (Location/Factors):** Missing categorical values are imputed using the **Mode** (most frequent value) or 'UNKNOWN' if a mode cannot be determined.\n",
    "\n",
    "### 2. Handling Outliers (Injuries/Fatalities)\n",
    "\n",
    "* **Method:** Outliers in count data (e.g., `NUMBER OF PERSONS INJURED`) are addressed using the **Interquartile Range (IQR)** method ($Q3 + 1.5 \\times IQR$).\n",
    "* **Decision:** Records with extreme outlier values are **removed**. This is justified because extreme counts are often recording errors, and removing them allows our analysis to focus on the reliable, common trends without statistical distortion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fd0d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_crashes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ensure df_crashes is available from Cell 1\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mdf_crashes\u001b[49m.copy()\n\u001b[32m      3\u001b[39m initial_shape = df.shape\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1. Handle Missing Values (Drop >50% Null, Impute Remaining)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_crashes' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensure df_crashes is available from Cell 1\n",
    "df = df_crashes.copy()\n",
    "initial_shape = df.shape\n",
    "\n",
    "# 1. Handle Missing Values (Drop >50% Null, Impute Remaining)\n",
    "null_threshold = len(df) * 0.5\n",
    "cols_to_drop = df.columns[df.isnull().sum() > null_threshold]\n",
    "df.drop(columns=cols_to_drop, inplace=True) \n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "for col in categorical_cols:\n",
    "     mode_val = df[col].mode()\n",
    "     fill_val = mode_val[0] if not mode_val.empty else \"unknown\"\n",
    "     df[col] = df[col].fillna(fill_val)\n",
    "\n",
    "# 2. Standardization and Conversion\n",
    "# --- FIX: Create a single, unified Datetime column for time-series plotting ---\n",
    "\n",
    "# 1. Combine 'CRASH DATE' (e.g., '11/11/2025') and 'CRASH TIME' (e.g., '14:30')\n",
    "df['CRASH_DATETIME_STR'] = df['CRASH DATE'] + ' ' + df['CRASH TIME']\n",
    "\n",
    "# 2. Convert the combined string to a single datetime object.\n",
    "# This column (CRASH_DATETIME) should be used as the X-axis for your line chart.\n",
    "df['CRASH_DATETIME'] = pd.to_datetime(df['CRASH_DATETIME_STR'], errors='coerce')\n",
    "\n",
    "# 3. Create helper columns from the unified column\n",
    "df['CRASH_YEAR'] = df['CRASH_DATETIME'].dt.year.astype('Int64')\n",
    "df['CRASH_MONTH'] = df['CRASH_DATETIME'].dt.to_period('M') # Useful for grouping by month\n",
    "\n",
    "# Drop the temporary string column and the redundant raw columns\n",
    "df.drop(columns=['CRASH_DATETIME_STR', 'CRASH DATE', 'CRASH TIME'], inplace=True, errors='ignore')\n",
    "\n",
    "for col in categorical_cols:\n",
    "     if col in df.columns:\n",
    "         df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "         \n",
    "# Store current state before outlier removal for validation\n",
    "df_before_outlier = df.copy()\n",
    "\n",
    "# 3. Handle Outliers (IQR Method)\n",
    "for col in ['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED']:\n",
    "     if col in df.columns:\n",
    "         Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "         IQR = Q3 - Q1\n",
    "         upper = Q3 + 1.5 * IQR\n",
    "         df = df[df[col] <= upper] \n",
    "\n",
    "# 4. Final Cleanup\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(subset=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
    "df_crashes_cleaned = df.copy() # Final output of cleaning\n",
    "\n",
    "print(\"\\nPre-Integration Cleaning Complete.\")\n",
    "\n",
    "# --- Validation Visualization ---\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df_before_outlier['NUMBER OF PERSONS INJURED'].dropna())\n",
    "plt.title('Before Outlier Removal (Raw Data)')\n",
    "plt.ylim(0, 10) \n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df_crashes_cleaned['NUMBER OF PERSONS INJURED'].dropna())\n",
    "plt.title('After Outlier Removal (Cleaned Data)')\n",
    "plt.ylim(0, 10) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b268bde",
   "metadata": {},
   "source": [
    "# 3. Data Integration and Post-Integration Cleaning\n",
    "\n",
    "This cell integrates the cleaned Crashes data with the Person dataset and performs final post-integration cleaning steps.\n",
    "\n",
    "## Justification for Integration Strategy (REQUIRED DOCUMENTATION)\n",
    "\n",
    "### 1. Secondary Dataset Choice\n",
    "We chose to integrate with the **Motor Vehicle Collisions Person** dataset.\n",
    "* **Rationale:** This dataset provides granular data on **injuries, fatalities, and demographics** (`PERSON_INJURY`, `PERSON_SEX`), which is directly necessary for answering the complex research questions (e.g., demographic impact on injury severity).\n",
    "\n",
    "### 2. Join Type and Key\n",
    "* **Key:** The merge is performed on the common unique identifier: **`COLLISION_ID`**.\n",
    "* **Join Type:** We use a **Left Join**. This ensures that the primary unit of analysis—**every cleaned crash record**—is preserved, even if there is no corresponding record in the Person dataset.\n",
    "\n",
    "### 3. Post-Integration Cleaning\n",
    "* **Redundancy Resolution:** We drop redundant location columns (e.g., `LATITUDE_PERSON`, `BOROUGH_PERSON`) because the primary Crashes table provides the definitive location information.\n",
    "* **New Missing Values:** Missing values in the new Person-specific columns (introduced by the Left Join) are imputed using **'UNKNOWN'** rather than being dropped. This preserves the crash record count and allows the dashboard to filter for \"unknown\" person details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb08c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_crashes_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ensure df_crashes_cleaned is available from Cell 2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_crashes_cleaned[\u001b[33m'\u001b[39m\u001b[33mCOLLISION_ID\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_crashes_cleaned\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mCOLLISION_ID\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m      3\u001b[39m persons_file = \u001b[33m'\u001b[39m\u001b[33mMotor_Vehicle_Collisions_-_Person_20251111.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# -------------------- Load and Preprocess Person Data --------------------\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_crashes_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensure df_crashes_cleaned is available from Cell 2\n",
    "df_crashes_cleaned['COLLISION_ID'] = df_crashes_cleaned['COLLISION_ID'].astype(str)\n",
    "persons_file = 'Motor_Vehicle_Collisions_-_Person_20251111.csv'\n",
    "\n",
    "# -------------------- Load and Preprocess Person Data --------------------\n",
    "\n",
    "try:\n",
    "    df_person = pd.read_csv(persons_file, low_memory=False)\n",
    "except FileNotFoundError:\n",
    "     print(f\"Error: Persons file '{persons_file}' not found.\")\n",
    "     df_person = pd.DataFrame(columns=['COLLISION_ID', 'PERSON_SEX', 'PERSON_INJURY']) \n",
    "\n",
    "# --- FIX: Aggregate Person Data before Joining to avoid losing person rows ---\n",
    "\n",
    "# Ensure ID is string type for joining\n",
    "df_person[\"COLLISION_ID\"] = df_person[\"COLLISION_ID\"].astype(str)\n",
    "df_person.dropna(subset=[\"COLLISION_ID\"], inplace=True)\n",
    "\n",
    "# Aggregate person-level data to the crash level\n",
    "person_summary = df_person.groupby('COLLISION_ID').agg(\n",
    "    # Count the number of unique persons involved per collision\n",
    "    PERSONS_INVOLVED_COUNT=('UNIQUE_ID', 'size'), # Assuming UNIQUE_ID or a similar column exists for count\n",
    "    # Tally the number of injured and killed persons using sum/count (adjust column names as needed)\n",
    "    KILLED_COUNT_PERSON=('PERSON_KILLED', 'sum'), # Assuming a binary column for killed/injured\n",
    "    INJURED_COUNT_PERSON=('PERSON_INJURY', lambda x: (x.str.lower() == 'injured').sum()) \n",
    "    # Use mode for categorical columns like sex\n",
    "    ,MOST_COMMON_SEX=('PERSON_SEX', lambda x: x.mode().iloc[0] if not x.mode().empty else 'UNKNOWN')\n",
    ").reset_index()\n",
    "\n",
    "# -------------------- Merge datasets (Left Join) --------------------\n",
    "\n",
    "merged = pd.merge(\n",
    "    df_crashes_cleaned,\n",
    "    person_summary,  # Use the aggregated summary\n",
    "    on=\"COLLISION_ID\",\n",
    "    how=\"left\", \n",
    "    # Remove suffixes as the Person columns are now aggregated summaries\n",
    ")\n",
    "\n",
    "print(f\"Integrated Shape: {merged.shape}\")\n",
    "\n",
    "# -------------------- Merge datasets (Left Join) --------------------\n",
    "\n",
    "merged = pd.merge(\n",
    "    df_crashes_cleaned,\n",
    "    df_person,\n",
    "    on=\"COLLISION_ID\",\n",
    "    how=\"left\", \n",
    "    suffixes=(\"_CRASH\", \"_PERSON\")\n",
    ")\n",
    "\n",
    "print(f\"Integrated Shape: {merged.shape}\")\n",
    "\n",
    "\n",
    "# -------------------- Post-Integration Cleaning --------------------\n",
    "\n",
    "# 1. Drop redundant location/date columns from the PERSON table\n",
    "redundant_cols = [\n",
    "    c for c in merged.columns if c.endswith('_PERSON') and \n",
    "    ('LATITUDE' in c or 'LONGITUDE' in c or 'BOROUGH' in c or 'CRASH DATE' in c)\n",
    "]\n",
    "merged.drop(columns=redundant_cols, inplace=True, errors='ignore')\n",
    "\n",
    "# 2. Handle new missing values (Person-specific columns) by filling 'UNKNOWN'\n",
    "cols_to_fill_unknown = ['PERSON_SEX', 'PERSON_INJURY', 'EJECTION', 'EMOTIONAL STATUS']\n",
    "for col in cols_to_fill_unknown:\n",
    "    if col in merged.columns:\n",
    "        merged[col] = merged[col].astype(str).str.lower().replace('nan', 'unknown')\n",
    "\n",
    "\n",
    "print(\"\\nPost-Integration Cleaning Complete.\")\n",
    "\n",
    "# -------------------- Save Final Output --------------------\n",
    "output_file_name = \"../merged_crashes_person.csv\"\n",
    "merged.to_csv(output_file_name, index=False)\n",
    "print(f\"\\n✅ Final cleaned and integrated dataset saved to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b3ddf",
   "metadata": {},
   "source": [
    "# 4. Project Research Questions & Conclusion\n",
    "\n",
    "## Proposed Research Questions (5-Member Team: 10 Questions)\n",
    "\n",
    "These questions are designed to be complex, utilizing the integrated data and spatial/temporal analysis, suitable for the interactive dashboard.\n",
    "\n",
    "1.  **Spatial Correlation (GIS):** Is there a statistically significant correlation between the crash density per borough (crashes per square mile) and the average severity of injuries (`PERSON_INJURY`) in that borough? (Team Member 1)\n",
    "2.  **Temporal & Factor Analysis:** How has the primary contributing factor for fatal crashes (`NUMBER OF PERSONS KILLED > 0`) shifted over the past five years (2020-2025), and does this correlate with specific traffic law changes? (Team Member 2)\n",
    "3.  **Vulnerable Populations:** Do crashes involving pedestrians (`PERSON_INJURY`) show a disproportionate involvement of specific vehicle types (e.g., trucks vs. passenger vehicles) during peak commuter hours (7-9 AM, 4-6 PM)? (Team Member 3)\n",
    "4.  **Weather/Time of Day:** Which combination of time-of-day (e.g., night vs. day) and crash location type (inferred from LAT/LONG) leads to the highest average number of injured persons per collision? (Team Member 4)\n",
    "5.  **Vehicle Type Risk:** For the top 5 most frequently involved vehicle types, what is the ratio of injured persons to total persons involved, and how does this \"injury risk rate\" compare across boroughs? (Team Member 1)\n",
    "6.  **Year-over-Year Fatality Trend:** What is the compound annual growth rate (CAGR) of fatalities (`NUMBER OF PERSONS KILLED`) for the top 3 most common contributing factors? (Team Member 2)\n",
    "7.  **Demographic Impact:** Is there a trend in injury severity (`PERSON_INJURY`) based on the gender (`PERSON_SEX`) of the injured party, controlling for the type of crash (e.g., sideswipe vs. rear-end)? (Team Member 3)\n",
    "8.  **Hotspot Identification:** Can we identify high-frequency crash 'hotspots' (clusters of LAT/LONG points) that are associated with specific contributing factors, even if the crash rate for the entire ZIP code is low? (Team Member 4)\n",
    "9.  **Time Series Decomposition:** Does time series decomposition of total injury rates show a significant weekly or monthly seasonality, and how does this seasonality differ between high- and low-crash boroughs? (Team Member 5)\n",
    "10. **Factor vs. Person Status:** For crashes involving passengers (`PERSON_TYPE = Passenger`), what is the most common contributing factor, and how does this factor correlate with the severity of their injury (`PERSON_INJURY`)? (Team Member 5)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The data engineering phase is complete. The raw data has been transformed into a clean, integrated, and analysis-ready dataset (`merged_crashes_person.csv`). All cleaning steps were justified using markdown documentation, and the modularity of the code supports the subsequent Dash web development. The next step is to finalize the deployment and ensure the interactive dashboard effectively visualizes the answers to these research questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
