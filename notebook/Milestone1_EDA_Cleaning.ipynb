{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697268c7",
   "metadata": {},
   "source": [
    "# NYC Traffic Crashes Analysis and Data Preparation\n",
    "\n",
    "This notebook performs comprehensive analysis of NYC traffic crashes data, preparing it for visualization in a web application. The analysis includes data cleaning, integration, and exploratory analysis of crash patterns across New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828b9d6",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Initial Inspection\n",
    "\n",
    "First, we'll import the required libraries and load our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b112769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 1. Exploratory Data Analysis (EDA)\n",
    "# This step involves loading the raw Crashes dataset and using descriptive statistics and initial analysis to understand its structure, issues (like missing data and inconsistencies), and initial patterns, fulfilling the thorough EDA requirement.\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- File paths (local) ---\n",
    "crashes_file = \"Motor_Vehicle_Collisions_-_Crashes_20251111.csv\"\n",
    "\n",
    "# --- Load raw data ---\n",
    "df_crashes_raw = pd.read_csv(crashes_file, low_memory=False)\n",
    "print(\"Initial Crashes Data Shape:\", df_crashes_raw.shape)\n",
    "\n",
    "# --- Initial Data Structure and Issues ---\n",
    "print(\"\\n--- Initial Data Info ---\")\n",
    "df_crashes_raw.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "# --- Missing Values Analysis ---\n",
    "# Show top columns with missing values (Critical for cleaning justification)\n",
    "missing_summary = df_crashes_raw.isnull().sum().sort_values(ascending=False).head(10)\n",
    "print(\"\\n--- Top 10 Missing Values ---\")\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# --- Descriptive Statistics (Numeric Data) ---\n",
    "print(\"\\n--- Descriptive Statistics for Injuries/Fatalities ---\")\n",
    "print(df_crashes_raw[['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED']].describe())\n",
    "\n",
    "# --- Initial Visualization (Example: Crashes by Borough) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "borough_counts = df_crashes_raw['BOROUGH'].fillna('UNKNOWN').value_counts()\n",
    "sns.barplot(x=borough_counts.index, y=borough_counts.values)\n",
    "plt.title('Initial Crash Counts by Borough')\n",
    "plt.ylabel('Number of Crashes')\n",
    "plt.xlabel('Borough')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Retain the raw data for the next cleaning step\n",
    "df_crashes = df_crashes_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394032f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 2. Pre-Integration Cleaning\n",
    "# This step involves cleaning the Crashes dataset before integration. We handle missing values, outliers, remove duplicates, and standardize formats. The resulting pre-cleaned data is saved to a temporary file (`pre_cleaned_crashes.csv`) to ensure data continuity for the next cell.\n",
    "\n",
    "# %%\n",
    "# Ensure the DataFrame df_crashes (from Cell 1) is available\n",
    "df = df_crashes.copy()\n",
    "initial_shape = df.shape\n",
    "\n",
    "# -------------------- Handle Missing Values (Pre-Integration) --------------------\n",
    "# [cite_start]1. Drop columns with too many missing values (e.g., > 50% missing) [cite: 50]\n",
    "df.dropna(axis=1, thresh=len(df) * 0.5, inplace=True) \n",
    "\n",
    "# [cite_start]2. Impute remaining missing values [cite: 50]\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute numeric: use Median (less sensitive to outliers)\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))\n",
    "# Impute categorical: use Mode (most frequent value)\n",
    "df[categorical_cols] = df[categorical_cols].apply(\n",
    "    lambda x: x.fillna(x.mode()[0] if not x.mode().empty else \"unknown\")\n",
    ")\n",
    "\n",
    "# -------------------- Remove Duplicates --------------------\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# -------------------- Standardize Formats --------------------\n",
    "# [cite_start]Convert date/time columns to datetime objects [cite: 52]\n",
    "datetime_cols = [col for col in df.columns if \"date\" in col.lower() or \"time\" in col.lower()]\n",
    "for col in datetime_cols:\n",
    "     df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# [cite_start]Standardize categorical strings (strip whitespace and lowercase) [cite: 52]\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "\n",
    "# -------------------- Handle Outliers (IQR) --------------------\n",
    "# [cite_start]Applied to counts of injuries/fatalities (using IQR method) [cite: 51]\n",
    "for col in ['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED']:\n",
    "    if col in df.columns:\n",
    "        Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        # Remove values above the upper fence\n",
    "        df = df[df[col] <= upper]\n",
    "\n",
    "# -------------------- Clean Location Data --------------------\n",
    "# Drop rows where LATITUDE or LONGITUDE is outside valid geographical ranges\n",
    "if {'LATITUDE', 'LONGITUDE'}.issubset(df.columns):\n",
    "    df = df[df['LATITUDE'].between(-90, 90) & df['LONGITUDE'].between(-180, 180)]\n",
    "\n",
    "print(\"Pre-Integration Cleaning Complete.\")\n",
    "print(f\"Rows Removed: {initial_shape[0] - df.shape[0]}\")\n",
    "print(f\"Final Pre-Cleaned Crashes Shape: {df.shape}\")\n",
    "\n",
    "# Save the pre-cleaned data for use in the next cell\n",
    "df.to_csv(\"pre_cleaned_crashes.csv\", index=False)\n",
    "print(\"✅ Pre-cleaned Crashes data saved to pre_cleaned_crashes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------- Load Person dataset in chunks --------------------\n",
    "person_file = \"Motor_Vehicle_Collisions_-_Person_20251111.csv\"\n",
    "chunksize = 100000  # Adjust based on your RAM\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(person_file, chunksize=chunksize, low_memory=False, on_bad_lines='skip'):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df_person = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# -------------------- Preprocess join key (Person) --------------------\n",
    "df_person = df_person.drop_duplicates(subset=[\"COLLISION_ID\"]).dropna(subset=[\"COLLISION_ID\"])\n",
    "df_person[\"COLLISION_ID\"] = df_person[\"COLLISION_ID\"].astype(str)\n",
    "df_crashes[\"COLLISION_ID\"] = df_crashes[\"COLLISION_ID\"].astype(str)\n",
    "\n",
    "# -------------------- Merge datasets (Integration) --------------------\n",
    "merged = pd.merge(\n",
    "    df_crashes,\n",
    "    df_person,\n",
    "    on=\"COLLISION_ID\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_CRASH\", \"_PERSON\")\n",
    ")\n",
    "\n",
    "# -------------------- Post-Integration Cleaning --------------------\n",
    "redundant_cols = [\n",
    "    c for c in merged.columns if c.endswith(\"_CRASH\") and c.replace(\"_CRASH\", \"_PERSON\") in merged.columns\n",
    "]\n",
    "redundant_cols.extend([c for c in merged.columns if c.startswith(('LATITUDE', 'LONGITUDE', 'BOROUGH')) and c.endswith('_PERSON')])\n",
    "\n",
    "merged.drop(columns=redundant_cols, inplace=True, errors='ignore')\n",
    "\n",
    "cols_to_fill_unknown = ['PERSON_SEX', 'PERSON_INJURY']\n",
    "for col in cols_to_fill_unknown:\n",
    "    if col in merged.columns:\n",
    "        merged[col] = merged[col].fillna('UNKNOWN')\n",
    "\n",
    "print(\"Post-Integration Cleaning Complete.\")\n",
    "print(f\"Final Integrated Shape: {merged.shape}\")\n",
    "print(\"Top missing values after post-cleaning:\\n\", merged.isna().sum().sort_values(ascending=False).head(5))\n",
    "\n",
    "# -------------------- Save Final Output --------------------\n",
    "output_file_name = \"merged_crashes_person.csv\"\n",
    "merged.to_csv(output_file_name, index=False)\n",
    "print(f\"\\n✅ Final cleaned and integrated dataset saved to {output_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
